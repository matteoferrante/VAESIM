Using cuda:0 device
Computing t-SNE to visualize from 32 to 2 dim - this could take a while..
Epoch 0: 100%|█| 3/3 [00:01<00:00,  2.56batch/s, loss=0.168, sim_loss=0.0286, recon_loss=0.139, kl_loss
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 128, 16, 16]           2,176
      ConvResBlock-2          [-1, 128, 16, 16]               0
            Conv2d-3            [-1, 256, 8, 8]         524,544
      ConvResBlock-4            [-1, 256, 8, 8]               0
            Conv2d-5            [-1, 384, 4, 4]       1,573,248
      ConvResBlock-6            [-1, 384, 4, 4]               0
           Flatten-7                 [-1, 6144]               0
            Linear-8                   [-1, 32]         196,640
            Linear-9                   [-1, 32]         196,640
       VAEEncoder-10       [[-1, 32], [-1, 32]]               0
           Linear-11                  [-1, 512]          16,896
        Unflatten-12             [-1, 32, 4, 4]               0
           Linear-13                   [-1, 16]             656
        Unflatten-14              [-1, 1, 4, 4]               0
  ConvTranspose2d-15            [-1, 384, 8, 8]         203,136
      BatchNorm2d-16            [-1, 384, 8, 8]             768
             GELU-17            [-1, 384, 8, 8]               0
ConvTransposeBlock-18            [-1, 384, 8, 8]               0
  ConvTranspose2d-19          [-1, 256, 16, 16]       1,573,120
      BatchNorm2d-20          [-1, 256, 16, 16]             512
             GELU-21          [-1, 256, 16, 16]               0
ConvTransposeBlock-22          [-1, 256, 16, 16]               0
  ConvTranspose2d-23          [-1, 128, 32, 32]         524,416
      BatchNorm2d-24          [-1, 128, 32, 32]             256
             GELU-25          [-1, 128, 32, 32]               0
ConvTransposeBlock-26          [-1, 128, 32, 32]               0
  ConvTranspose2d-27            [-1, 1, 32, 32]           1,153
          Sigmoid-28            [-1, 1, 32, 32]               0
         cDecoder-29            [-1, 1, 32, 32]               0
================================================================
Total params: 4,814,161
Trainable params: 4,814,161
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 7.66
Params size (MB): 18.36
Estimated Total Size (MB): 26.03
----------------------------------------------------------------
Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz
Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz
Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz
/home/matteo/Unsupervised/vaesim_baselines/VAESIM/../../NeuroGEN_Pytorch/utils/callbacks.py:158: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  c=torch.nn.functional.softmax(logprobs/0.25)
Epoch 1: 100%|█| 3/3 [00:01<00:00,  2.88batch/s, loss=0.552, sim_loss=0.444, recon_loss=0.103, kl_loss=
Epoch 2: 100%|█| 3/3 [00:01<00:00,  2.85batch/s, loss=0.503, sim_loss=0.427, recon_loss=0.0642, kl_loss
Epoch 3: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.481, sim_loss=0.423, recon_loss=0.048, kl_loss=
Epoch 4: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.46, sim_loss=0.414, recon_loss=0.0371, kl_loss=
Epoch 5: 100%|█| 3/3 [00:01<00:00,  2.94batch/s, loss=0.426, sim_loss=0.383, recon_loss=0.033, kl_loss=
Epoch 6: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.38, sim_loss=0.342, recon_loss=0.0284, kl_loss=
Epoch 7: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.357, sim_loss=0.322, recon_loss=0.0258, kl_loss
Epoch 8: 100%|█| 3/3 [00:01<00:00,  2.95batch/s, loss=0.324, sim_loss=0.292, recon_loss=0.0241, kl_loss
Epoch 9: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.25, sim_loss=0.22, recon_loss=0.0225, kl_loss=0
Epoch 10: 100%|█| 3/3 [00:01<00:00,  2.92batch/s, loss=0.249, sim_loss=0.22, recon_loss=0.0218, kl_loss
Epoch 11: 100%|█| 3/3 [00:01<00:00,  2.65batch/s, loss=0.225, sim_loss=0.198, recon_loss=0.0201, kl_los
Epoch 12: 100%|█| 3/3 [00:01<00:00,  2.98batch/s, loss=0.189, sim_loss=0.162, recon_loss=0.0197, kl_los
Epoch 13: 100%|█| 3/3 [00:01<00:00,  2.97batch/s, loss=0.179, sim_loss=0.153, recon_loss=0.0191, kl_los
Epoch 14: 100%|█| 3/3 [00:01<00:00,  2.93batch/s, loss=0.142, sim_loss=0.117, recon_loss=0.0185, kl_los
Epoch 15: 100%|█| 3/3 [00:00<00:00,  3.16batch/s, loss=0.121, sim_loss=0.0969, recon_loss=0.0178, kl_lo
Epoch 16: 100%|█| 3/3 [00:01<00:00,  2.91batch/s, loss=0.0907, sim_loss=0.067, recon_loss=0.0172, kl_lo
Epoch 17: 100%|█| 3/3 [00:01<00:00,  2.97batch/s, loss=0.0853, sim_loss=0.0626, recon_loss=0.0165, kl_l
Epoch 18: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.0825, sim_loss=0.0597, recon_loss=0.017, kl_lo
Epoch 19: 100%|█| 3/3 [00:01<00:00,  2.94batch/s, loss=0.0658, sim_loss=0.0437, recon_loss=0.0165, kl_l
Epoch 20: 100%|█| 3/3 [00:01<00:00,  2.95batch/s, loss=0.0677, sim_loss=0.046, recon_loss=0.0162, kl_lo
Epoch 21: 100%|█| 3/3 [00:01<00:00,  2.87batch/s, loss=0.0648, sim_loss=0.0431, recon_loss=0.0161, kl_l
Epoch 22: 100%|█| 3/3 [00:00<00:00,  3.01batch/s, loss=0.0649, sim_loss=0.0433, recon_loss=0.0162, kl_l
Epoch 23: 100%|█| 3/3 [00:01<00:00,  2.73batch/s, loss=0.0625, sim_loss=0.0412, recon_loss=0.0159, kl_l
Epoch 24: 100%|█| 3/3 [00:00<00:00,  3.09batch/s, loss=0.061, sim_loss=0.0402, recon_loss=0.0153, kl_lo
Epoch 25: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.0618, sim_loss=0.0406, recon_loss=0.0159, kl_l
Epoch 26: 100%|█| 3/3 [00:00<00:00,  3.01batch/s, loss=0.0648, sim_loss=0.0438, recon_loss=0.0156, kl_l
Epoch 27: 100%|█| 3/3 [00:00<00:00,  3.00batch/s, loss=0.0655, sim_loss=0.0446, recon_loss=0.0155, kl_l
Epoch 28: 100%|█| 3/3 [00:00<00:00,  3.04batch/s, loss=0.0626, sim_loss=0.0419, recon_loss=0.0152, kl_l
Epoch 29: 100%|█| 3/3 [00:01<00:00,  2.98batch/s, loss=0.0689, sim_loss=0.0486, recon_loss=0.015, kl_lo
Epoch 30: 100%|█| 3/3 [00:00<00:00,  3.03batch/s, loss=0.0636, sim_loss=0.0433, recon_loss=0.0148, kl_l
Epoch 31: 100%|█| 3/3 [00:01<00:00,  2.94batch/s, loss=0.074, sim_loss=0.0537, recon_loss=0.0149, kl_lo
Epoch 32: 100%|█| 3/3 [00:01<00:00,  2.99batch/s, loss=0.0649, sim_loss=0.0445, recon_loss=0.0152, kl_l
Epoch 33: 100%|█| 3/3 [00:00<00:00,  3.02batch/s, loss=0.0655, sim_loss=0.0458, recon_loss=0.0144, kl_l
Epoch 34: 100%|█| 3/3 [00:01<00:00,  2.72batch/s, loss=0.0713, sim_loss=0.0514, recon_loss=0.0148, kl_l
Epoch 35: 100%|█| 3/3 [00:00<00:00,  3.02batch/s, loss=0.0664, sim_loss=0.0467, recon_loss=0.0145, kl_l
Epoch 36: 100%|█| 3/3 [00:01<00:00,  2.90batch/s, loss=0.0711, sim_loss=0.0519, recon_loss=0.0138, kl_l
Epoch 37: 100%|█| 3/3 [00:01<00:00,  2.98batch/s, loss=0.0746, sim_loss=0.0553, recon_loss=0.014, kl_lo
Epoch 38: 100%|█| 3/3 [00:00<00:00,  3.20batch/s, loss=0.0721, sim_loss=0.0527, recon_loss=0.0145, kl_l
Epoch 39: 100%|█| 3/3 [00:00<00:00,  3.03batch/s, loss=0.0796, sim_loss=0.0601, recon_loss=0.0143, kl_l
Epoch 40: 100%|█| 3/3 [00:01<00:00,  2.98batch/s, loss=0.0713, sim_loss=0.0519, recon_loss=0.0141, kl_l
Epoch 41: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.0596, sim_loss=0.0408, recon_loss=0.0138, kl_l
Epoch 42: 100%|█| 3/3 [00:00<00:00,  3.18batch/s, loss=0.0692, sim_loss=0.0501, recon_loss=0.0139, kl_l
Epoch 43: 100%|█| 3/3 [00:01<00:00,  2.99batch/s, loss=0.0651, sim_loss=0.0461, recon_loss=0.014, kl_lo
Epoch 44: 100%|█| 3/3 [00:00<00:00,  3.02batch/s, loss=0.0638, sim_loss=0.0449, recon_loss=0.0139, kl_l
Epoch 45: 100%|█| 3/3 [00:01<00:00,  2.66batch/s, loss=0.0636, sim_loss=0.0443, recon_loss=0.0142, kl_l
Epoch 46: 100%|█| 3/3 [00:01<00:00,  2.98batch/s, loss=0.0734, sim_loss=0.0545, recon_loss=0.0139, kl_l
Epoch 47: 100%|█| 3/3 [00:00<00:00,  3.19batch/s, loss=0.0693, sim_loss=0.0506, recon_loss=0.0135, kl_l
Epoch 48: 100%|█| 3/3 [00:00<00:00,  3.00batch/s, loss=0.0672, sim_loss=0.0485, recon_loss=0.0135, kl_l
Epoch 49: 100%|█| 3/3 [00:01<00:00,  2.99batch/s, loss=0.0705, sim_loss=0.0519, recon_loss=0.0136, kl_l
Epoch 50: 100%|█| 3/3 [00:01<00:00,  2.99batch/s, loss=0.0708, sim_loss=0.0519, recon_loss=0.014, kl_lo
Epoch 51: 100%|█| 3/3 [00:00<00:00,  3.02batch/s, loss=0.0654, sim_loss=0.0465, recon_loss=0.0137, kl_l
Epoch 52: 100%|█| 3/3 [00:01<00:00,  3.00batch/s, loss=0.0757, sim_loss=0.0571, recon_loss=0.0135, kl_l
Epoch 53: 100%|█| 3/3 [00:00<00:00,  3.03batch/s, loss=0.0695, sim_loss=0.0508, recon_loss=0.0136, kl_l
Epoch 54: 100%|█| 3/3 [00:00<00:00,  3.01batch/s, loss=0.0751, sim_loss=0.056, recon_loss=0.0142, kl_lo
Epoch 55: 100%|█| 3/3 [00:00<00:00,  3.01batch/s, loss=0.0753, sim_loss=0.0562, recon_loss=0.0139, kl_l
Epoch 56: 100%|█| 3/3 [00:01<00:00,  2.53batch/s, loss=0.0677, sim_loss=0.0487, recon_loss=0.014, kl_lo
Epoch 57: 100%|█| 3/3 [00:01<00:00,  2.95batch/s, loss=0.0726, sim_loss=0.0536, recon_loss=0.0139, kl_l
Epoch 58: 100%|█| 3/3 [00:01<00:00,  2.93batch/s, loss=0.0833, sim_loss=0.0646, recon_loss=0.0136, kl_l
Epoch 59: 100%|█| 3/3 [00:00<00:00,  3.05batch/s, loss=0.079, sim_loss=0.0606, recon_loss=0.0134, kl_lo
100%|████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.74it/s]
100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.86it/s]