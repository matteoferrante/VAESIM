
Using cuda:0 device
Computing t-SNE to visualize from 32 to 2 dim - this could take a while..
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 128, 16, 16]           2,176
      ConvResBlock-2          [-1, 128, 16, 16]               0
            Conv2d-3            [-1, 256, 8, 8]         524,544
      ConvResBlock-4            [-1, 256, 8, 8]               0
            Conv2d-5            [-1, 384, 4, 4]       1,573,248
      ConvResBlock-6            [-1, 384, 4, 4]               0
           Flatten-7                 [-1, 6144]               0
            Linear-8                   [-1, 32]         196,640
            Linear-9                   [-1, 32]         196,640
       VAEEncoder-10       [[-1, 32], [-1, 32]]               0
           Linear-11                  [-1, 512]          16,896
        Unflatten-12             [-1, 32, 4, 4]               0
           Linear-13                   [-1, 16]             656
        Unflatten-14              [-1, 1, 4, 4]               0
  ConvTranspose2d-15            [-1, 384, 8, 8]         203,136
      BatchNorm2d-16            [-1, 384, 8, 8]             768
             GELU-17            [-1, 384, 8, 8]               0
ConvTransposeBlock-18            [-1, 384, 8, 8]               0
  ConvTranspose2d-19          [-1, 256, 16, 16]       1,573,120
      BatchNorm2d-20          [-1, 256, 16, 16]             512
             GELU-21          [-1, 256, 16, 16]               0
ConvTransposeBlock-22          [-1, 256, 16, 16]               0
  ConvTranspose2d-23          [-1, 128, 32, 32]         524,416
      BatchNorm2d-24          [-1, 128, 32, 32]             256
             GELU-25          [-1, 128, 32, 32]               0
ConvTransposeBlock-26          [-1, 128, 32, 32]               0
  ConvTranspose2d-27            [-1, 1, 32, 32]           1,153
          Sigmoid-28            [-1, 1, 32, 32]               0
         cDecoder-29            [-1, 1, 32, 32]               0
================================================================
Total params: 4,814,161
Trainable params: 4,814,161
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 7.66
Params size (MB): 18.36
Estimated Total Size (MB): 26.03
----------------------------------------------------------------
Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz
Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz
Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz
Epoch 0: 100%|█| 3/3 [00:01<00:00,  2.90batch/s, loss=0.186, sim_loss=0.0269, recon_loss=0.158, kl_loss
/home/matteo/Unsupervised/vaesim_baselines/VAESIM/../../NeuroGEN_Pytorch/utils/callbacks.py:158: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  c=torch.nn.functional.softmax(logprobs/0.25)
Epoch 1: 100%|█| 3/3 [00:00<00:00,  3.08batch/s, loss=0.56, sim_loss=0.439, recon_loss=0.116, kl_loss=0
Epoch 2: 100%|█| 3/3 [00:00<00:00,  3.09batch/s, loss=0.57, sim_loss=0.491, recon_loss=0.0644, kl_loss=
Epoch 3: 100%|█| 3/3 [00:00<00:00,  3.20batch/s, loss=0.543, sim_loss=0.483, recon_loss=0.0496, kl_loss
Epoch 4: 100%|█| 3/3 [00:00<00:00,  3.12batch/s, loss=0.513, sim_loss=0.461, recon_loss=0.0429, kl_loss
Epoch 5: 100%|█| 3/3 [00:00<00:00,  3.17batch/s, loss=0.508, sim_loss=0.463, recon_loss=0.0344, kl_loss
Epoch 6: 100%|█| 3/3 [00:00<00:00,  3.31batch/s, loss=0.5, sim_loss=0.461, recon_loss=0.0296, kl_loss=0
Epoch 7: 100%|█| 3/3 [00:00<00:00,  3.39batch/s, loss=0.494, sim_loss=0.459, recon_loss=0.0257, kl_loss
Epoch 8: 100%|█| 3/3 [00:00<00:00,  3.37batch/s, loss=0.462, sim_loss=0.429, recon_loss=0.024, kl_loss=
Epoch 9: 100%|█| 3/3 [00:00<00:00,  3.41batch/s, loss=0.47, sim_loss=0.44, recon_loss=0.0213, kl_loss=0
Epoch 10: 100%|█| 3/3 [00:00<00:00,  3.42batch/s, loss=0.468, sim_loss=0.44, recon_loss=0.0204, kl_loss
Epoch 11: 100%|█| 3/3 [00:00<00:00,  3.09batch/s, loss=0.464, sim_loss=0.436, recon_loss=0.0196, kl_los
Epoch 12: 100%|█| 3/3 [00:00<00:00,  3.55batch/s, loss=0.462, sim_loss=0.436, recon_loss=0.0183, kl_los
Epoch 13: 100%|█| 3/3 [00:00<00:00,  3.46batch/s, loss=0.424, sim_loss=0.398, recon_loss=0.0178, kl_los
Epoch 14: 100%|█| 3/3 [00:00<00:00,  3.41batch/s, loss=0.423, sim_loss=0.398, recon_loss=0.0169, kl_los
Epoch 15: 100%|█| 3/3 [00:00<00:00,  3.42batch/s, loss=0.401, sim_loss=0.376, recon_loss=0.0171, kl_los
Epoch 16: 100%|█| 3/3 [00:00<00:00,  3.41batch/s, loss=0.395, sim_loss=0.371, recon_loss=0.0173, kl_los
Epoch 17: 100%|█| 3/3 [00:00<00:00,  3.51batch/s, loss=0.365, sim_loss=0.342, recon_loss=0.0163, kl_los
Epoch 18: 100%|█| 3/3 [00:00<00:00,  3.45batch/s, loss=0.343, sim_loss=0.319, recon_loss=0.0165, kl_los
Epoch 19: 100%|█| 3/3 [00:00<00:00,  3.21batch/s, loss=0.322, sim_loss=0.299, recon_loss=0.0157, kl_los
Epoch 20: 100%|█| 3/3 [00:00<00:00,  3.18batch/s, loss=0.294, sim_loss=0.273, recon_loss=0.015, kl_loss
Epoch 21: 100%|█| 3/3 [00:00<00:00,  3.20batch/s, loss=0.288, sim_loss=0.267, recon_loss=0.0142, kl_los
Epoch 22: 100%|█| 3/3 [00:00<00:00,  3.20batch/s, loss=0.259, sim_loss=0.238, recon_loss=0.0143, kl_los
Epoch 23: 100%|█| 3/3 [00:00<00:00,  3.20batch/s, loss=0.228, sim_loss=0.207, recon_loss=0.0143, kl_los
Epoch 24: 100%|█| 3/3 [00:00<00:00,  3.26batch/s, loss=0.182, sim_loss=0.161, recon_loss=0.0143, kl_los
Epoch 25: 100%|█| 3/3 [00:00<00:00,  3.21batch/s, loss=0.179, sim_loss=0.158, recon_loss=0.0142, kl_los
Epoch 26: 100%|█| 3/3 [00:00<00:00,  3.22batch/s, loss=0.181, sim_loss=0.161, recon_loss=0.0142, kl_los
Epoch 27: 100%|█| 3/3 [00:00<00:00,  3.19batch/s, loss=0.182, sim_loss=0.161, recon_loss=0.0141, kl_los
Epoch 28: 100%|█| 3/3 [00:00<00:00,  3.12batch/s, loss=0.188, sim_loss=0.168, recon_loss=0.0134, kl_los
Epoch 29: 100%|█| 3/3 [00:00<00:00,  3.20batch/s, loss=0.18, sim_loss=0.159, recon_loss=0.014, kl_loss=
Epoch 30: 100%|█| 3/3 [00:00<00:00,  3.34batch/s, loss=0.176, sim_loss=0.156, recon_loss=0.0135, kl_los
Epoch 31: 100%|█| 3/3 [00:00<00:00,  3.18batch/s, loss=0.162, sim_loss=0.142, recon_loss=0.0135, kl_los
Epoch 32: 100%|█| 3/3 [00:00<00:00,  3.24batch/s, loss=0.143, sim_loss=0.123, recon_loss=0.0132, kl_los
Epoch 33: 100%|█| 3/3 [00:01<00:00,  2.87batch/s, loss=0.132, sim_loss=0.112, recon_loss=0.0139, kl_los
Epoch 34: 100%|█| 3/3 [00:00<00:00,  3.25batch/s, loss=0.134, sim_loss=0.115, recon_loss=0.0134, kl_los
Epoch 35: 100%|█| 3/3 [00:00<00:00,  3.21batch/s, loss=0.12, sim_loss=0.1, recon_loss=0.0136, kl_loss=0
Epoch 36: 100%|█| 3/3 [00:00<00:00,  3.39batch/s, loss=0.108, sim_loss=0.0883, recon_loss=0.0132, kl_lo
Epoch 37: 100%|█| 3/3 [00:00<00:00,  3.39batch/s, loss=0.105, sim_loss=0.0851, recon_loss=0.0138, kl_lo
Epoch 38: 100%|█| 3/3 [00:00<00:00,  3.43batch/s, loss=0.0993, sim_loss=0.0796, recon_loss=0.0131, kl_l
Epoch 39: 100%|█| 3/3 [00:00<00:00,  3.28batch/s, loss=0.0977, sim_loss=0.0781, recon_loss=0.0132, kl_l
Epoch 40: 100%|█| 3/3 [00:00<00:00,  3.34batch/s, loss=0.0962, sim_loss=0.0768, recon_loss=0.0131, kl_l
Epoch 41: 100%|█| 3/3 [00:00<00:00,  3.37batch/s, loss=0.0895, sim_loss=0.0702, recon_loss=0.0129, kl_l
Epoch 42: 100%|█| 3/3 [00:00<00:00,  3.34batch/s, loss=0.0805, sim_loss=0.0607, recon_loss=0.0135, kl_l
Epoch 43: 100%|█| 3/3 [00:00<00:00,  3.13batch/s, loss=0.0707, sim_loss=0.0511, recon_loss=0.0133, kl_l
Epoch 44: 100%|█| 3/3 [00:00<00:00,  3.24batch/s, loss=0.0712, sim_loss=0.0515, recon_loss=0.0135, kl_l
Epoch 45: 100%|█| 3/3 [00:01<00:00,  2.86batch/s, loss=0.067, sim_loss=0.0473, recon_loss=0.0135, kl_lo
Epoch 46: 100%|█| 3/3 [00:00<00:00,  3.15batch/s, loss=0.0621, sim_loss=0.0425, recon_loss=0.0132, kl_l
Epoch 47: 100%|█| 3/3 [00:00<00:00,  3.21batch/s, loss=0.0617, sim_loss=0.0426, recon_loss=0.0128, kl_l
Epoch 48: 100%|█| 3/3 [00:01<00:00,  2.93batch/s, loss=0.062, sim_loss=0.0424, recon_loss=0.0134, kl_lo
Epoch 49: 100%|█| 3/3 [00:01<00:00,  2.95batch/s, loss=0.0606, sim_loss=0.0412, recon_loss=0.013, kl_lo
Epoch 50: 100%|█| 3/3 [00:01<00:00,  2.93batch/s, loss=0.0592, sim_loss=0.0399, recon_loss=0.013, kl_lo
Epoch 51: 100%|█| 3/3 [00:01<00:00,  2.96batch/s, loss=0.0594, sim_loss=0.0401, recon_loss=0.013, kl_lo
Epoch 52: 100%|█| 3/3 [00:01<00:00,  2.93batch/s, loss=0.0568, sim_loss=0.0375, recon_loss=0.0131, kl_l
Epoch 53: 100%|█| 3/3 [00:00<00:00,  3.01batch/s, loss=0.0584, sim_loss=0.0393, recon_loss=0.013, kl_lo
Epoch 54: 100%|█| 3/3 [00:01<00:00,  2.91batch/s, loss=0.0595, sim_loss=0.0401, recon_loss=0.0131, kl_l
Epoch 55: 100%|█| 3/3 [00:01<00:00,  2.99batch/s, loss=0.0577, sim_loss=0.0386, recon_loss=0.0128, kl_l
Epoch 56: 100%|█| 3/3 [00:01<00:00,  2.93batch/s, loss=0.059, sim_loss=0.0398, recon_loss=0.013, kl_los
Epoch 57: 100%|█| 3/3 [00:01<00:00,  2.64batch/s, loss=0.0606, sim_loss=0.0413, recon_loss=0.0131, kl_l
Epoch 58: 100%|█| 3/3 [00:01<00:00,  2.87batch/s, loss=0.0588, sim_loss=0.0393, recon_loss=0.0133, kl_l
Epoch 59: 100%|█| 3/3 [00:01<00:00,  2.94batch/s, loss=0.0603, sim_loss=0.0409, recon_loss=0.013, kl_lo
100%|████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  6.71it/s]
100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.28it/s]