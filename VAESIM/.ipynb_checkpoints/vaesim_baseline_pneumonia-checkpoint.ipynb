{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1a8e57-70c9-4849-a416-b05244f05bfa",
   "metadata": {},
   "source": [
    "### VAESIM v6\n",
    "\n",
    "1) Build on best performances from sweep\n",
    "2) Introduce temperature scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe72fd53-f5a3-43cd-9688-597871319d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (0.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (from opencv-python) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchextractor in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: numpy in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (from torchextractor) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (from torchextractor) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (from torch>=1.4.0->torchextractor) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: omegaconf in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (from omegaconf) (6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages (from omegaconf) (4.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imutils\n",
    "%pip install opencv-python\n",
    "%pip install torchextractor\n",
    "%pip install omegaconf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be80327b-e561-451d-b5de-d1fb67c7e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/Unsupervised/vaesim_baselines/VAESIM/../../NeuroGEN_Pytorch/utils/utils.py:134: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteoferrante\u001b[0m (\u001b[33mtorvergatafmri\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../NeuroGEN_Pytorch/\")\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "from classes.Architectures import VAEDecoder, VAEEncoder, Discriminator,cVAEDecoder\n",
    "from classes.Cluster import VAESIM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST,CIFAR10\n",
    "from torchvision.transforms import Compose,ToTensor,Resize,PILToTensor\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch import distributions as D\n",
    "from torch.nn.functional import softmax\n",
    "import wandb\n",
    "\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from utils.callbacks import *\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from utils.utils import linear_assignment\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "#from coclust.evaluation.external import accuracy\n",
    "import pandas as pd\n",
    "wandb.login()\n",
    "from evaluations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6687776-226f-4519-b714-206cc36475f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,latent_dim=50,n_conv=3,n_init_filters=32,input_channels=1):\n",
    "        super().__init__()\n",
    "        layers=[]\n",
    "        for i in range(n_conv):\n",
    "            if i==0:\n",
    "                layers.append(nn.Conv2d(input_channels,n_init_filters,kernel_size=4,stride=2,padding=1))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.BatchNorm2d(n_init_filters))\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(n_init_filters*(2**(i-1)),n_init_filters*2**i,kernel_size=4,stride=2,padding=1))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.BatchNorm2d(n_init_filters*2**i))\n",
    "        layers.append(nn.Flatten())\n",
    "        layers.append(nn.LazyLinear(latent_dim))\n",
    "        \n",
    "        self.network=nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.network(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b321274-f756-4890-8610-1d825b29aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=50,target_shape=(1,28,28) , n_conv=2, n_init_filters=64, condition_dim=10):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.condition_dim=condition_dim\n",
    "\n",
    "\n",
    "\n",
    "        # infer the starting dimension.\n",
    "        target_shape_side = target_shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "        self.startDim = target_shape_side // (2 ** n_conv)\n",
    "\n",
    "        self.n_init_filters=n_init_filters\n",
    "        \n",
    "        #self.predecoder=nn.Unflatten(self.first_channels*self.startDim*self.startDim)\n",
    "        self.predecoder=nn.Linear(latent_dim,self.n_init_filters*self.startDim*self.startDim)\n",
    "        self.unflatten=nn.Unflatten(1,(self.n_init_filters,self.startDim,self.startDim))\n",
    "\n",
    "        self.condition =  nn.Linear(self.condition_dim,self.startDim*self.startDim)\n",
    "        self.condition2shape = nn.Unflatten(1, (1,self.startDim , self.startDim))\n",
    "        feature_layers = []\n",
    "        for i in range(n_conv):\n",
    "            if i==0:\n",
    "                feature_layers.append(nn.LazyConvTranspose2d(n_init_filters,kernel_size=4,stride=2,padding=1))\n",
    "            else:\n",
    "                feature_layers.append(nn.ConvTranspose2d(n_init_filters*(2**(i-1)),n_init_filters*2**i,kernel_size=4,stride=2,padding=1))\n",
    "\n",
    "        self.features = nn.Sequential(*feature_layers)\n",
    "\n",
    "        self.decoder_output=nn.LazyConvTranspose2d(target_shape[0],3,padding=1)\n",
    "        self.activation=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x,c):\n",
    "        x = self.predecoder(x)\n",
    "        x= self.unflatten(x)\n",
    "\n",
    "        c= self.condition(c)\n",
    "        c= self.condition2shape(c)\n",
    "        \n",
    "        x= torch.concat((x,c),axis=1)\n",
    "        x = x.view(x.shape[0], -1, self.startDim, self.startDim)\n",
    "        x = self.features(x)\n",
    "        x = self.decoder_output(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643b18d-3ab0-405b-b8a7-2a181cf0b9de",
   "metadata": {},
   "source": [
    "#### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3a8f9d-324f-487d-a300-1f351764565e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running model with KL: True, similarity loss: True and sampling: False temperature: 5 reinitilize: 0.0 ema: 0.95 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matteo/Unsupervised/vaesim_baselines/VAESIM/wandb/run-20220812_225459-236ca0xu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/torvergatafmri/NeuroGEN_Pytorch/runs/236ca0xu\" target=\"_blank\">confused-forest-11</a></strong> to <a href=\"https://wandb.ai/torvergatafmri/NeuroGEN_Pytorch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/torvergatafmri/NeuroGEN_Pytorch/runs/236ca0xu?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f166a8fdf70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_basis=40\n",
    "latent_dim=32\n",
    "input_dim=(1,32,32)\n",
    "\n",
    "sample_cluster=False\n",
    "kl_weight=5e-3\n",
    "sim_weight=0.1\n",
    "similarity=True\n",
    "kl=True\n",
    "reinit=0.\n",
    "\n",
    "temperature=5\n",
    "\n",
    "## using VAEEncoder\n",
    "encoder_architecture=[[0,64],[0,128],[0,256]]\n",
    "\n",
    "e=VAEEncoder( latent_dim=latent_dim,  conv_layer_list=encoder_architecture)\n",
    "d=decoder(latent_dim=latent_dim,condition_dim=n_basis,n_init_filters=128,target_shape=input_dim)\n",
    "\n",
    "\n",
    "model=VAESIM(input_dim=input_dim,latent_dim=latent_dim,encoder=e,decoder=d,n_basis=n_basis,weight=kl_weight,sim_weight=sim_weight,similarity=similarity,kl=kl,sample_cluster=sample_cluster,reinit=reinit,temperature=temperature,schedule=True,ema=0.95)\n",
    "\n",
    "\n",
    "EPOCHS=20\n",
    "BS=2000\n",
    "INIT_LR=1e-4\n",
    "\n",
    "config={\"dataset\":\"MNIST10\", \"type\":\"VAESIM\",\"latent_dim\":latent_dim, \"n_basis\":n_basis, \"input_dim\":input_dim}\n",
    "config[\"epochs\"]=EPOCHS\n",
    "config[\"BS\"]=BS\n",
    "config[\"init_lr\"]=INIT_LR\n",
    "\n",
    "config[\"sample_cluster\"]=sample_cluster\n",
    "config[\"use_kl_loss\"]=kl\n",
    "config[\"use_similarity_loss\"]=similarity\n",
    "config[\"reinitialization_probability\"]=reinit\n",
    "config[\"kl_weight\"]=kl_weight\n",
    "config[\"sim_weight\"]=sim_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project=\"NeuroGEN_Pytorch\",config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440aa8b2-3ccd-4fe1-bc37-e44d0bf36a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d1bc-604f-4f92-abe3-44e250582c12",
   "metadata": {},
   "source": [
    "## Run this cell to Init the basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d463388-e245-4c2c-85a3-da6b9a3c266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing t-SNE to visualize from 32 to 2 dim - this could take a while..\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=torch.rand(BS,*input_dim).to(device)\n",
    "z_mean,z_log_var=model.encoder(x)\n",
    "q_z = D.normal.Normal(z_mean, torch.exp(0.5 * z_log_var))\n",
    "z=q_z.rsample()\n",
    "\n",
    "model.init_basis(z.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61d8df1-d093-431e-baa2-8081bf25e870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           1,088\n",
      "      ConvResBlock-2           [-1, 64, 16, 16]               0\n",
      "            Conv2d-3            [-1, 128, 8, 8]         131,200\n",
      "      ConvResBlock-4            [-1, 128, 8, 8]               0\n",
      "            Conv2d-5            [-1, 256, 4, 4]         524,544\n",
      "      ConvResBlock-6            [-1, 256, 4, 4]               0\n",
      "           Flatten-7                 [-1, 4096]               0\n",
      "            Linear-8                   [-1, 32]         131,104\n",
      "            Linear-9                   [-1, 32]         131,104\n",
      "       VAEEncoder-10       [[-1, 32], [-1, 32]]               0\n",
      "           Linear-11                 [-1, 8192]         270,336\n",
      "        Unflatten-12            [-1, 128, 8, 8]               0\n",
      "           Linear-13                   [-1, 64]           2,624\n",
      "        Unflatten-14              [-1, 1, 8, 8]               0\n",
      "  ConvTranspose2d-15          [-1, 128, 16, 16]         264,320\n",
      "  ConvTranspose2d-16          [-1, 256, 32, 32]         524,544\n",
      "  ConvTranspose2d-17            [-1, 1, 32, 32]           2,305\n",
      "          Sigmoid-18            [-1, 1, 32, 32]               0\n",
      "          decoder-19            [-1, 1, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 1,983,169\n",
      "Trainable params: 1,983,169\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.86\n",
      "Params size (MB): 7.57\n",
      "Estimated Total Size (MB): 10.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f947852-015f-4410-b7ad-b8abcbc9cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=Compose([Resize(32),ToTensor()])\n",
    "\n",
    "data_flag = 'pneumoniamnist'\n",
    "# data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "#train_dataloader=DataLoader(train_dataset,batch_size=BS,shuffle=True)\n",
    "#test_dataloader=DataLoader(test_dataset,batch_size=BS,shuffle=False)\n",
    "\n",
    "train_dataset = DataClass(split='train', transform=transform, download=download)\n",
    "val_dataset = DataClass(split='val', transform=transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=transform, download=download)\n",
    "\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=BS,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=BS,shuffle=True)\n",
    "\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=BS,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31badca1-2e5f-4dfa-bdda-c10aa7b6ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"models/vaesim_mnist\"\n",
    "os.makedirs(base_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb1426-6388-425f-81e5-91b8b585e185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4a121-c950-4f92-bd49-757153926aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 30/30 [00:12<00:00,  2.36batch/s, loss=0.409, sim_loss=0.347, recon_loss=0.06, kl_loss\n",
      "/home/matteo/Unsupervised/vaesim_baselines/VAESIM/../../NeuroGEN_Pytorch/utils/callbacks.py:158: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  c=torch.nn.functional.softmax(logprobs/0.25)\n",
      "Epoch 1: 100%|█| 30/30 [00:12<00:00,  2.39batch/s, loss=0.241, sim_loss=0.199, recon_loss=0.0371, kl_lo\n",
      "Epoch 2: 100%|█| 30/30 [00:11<00:00,  2.52batch/s, loss=0.188, sim_loss=0.156, recon_loss=0.0264, kl_lo\n",
      "Epoch 3: 100%|█| 30/30 [00:11<00:00,  2.55batch/s, loss=0.186, sim_loss=0.159, recon_loss=0.0208, kl_lo\n",
      "Epoch 4: 100%|█| 30/30 [00:12<00:00,  2.40batch/s, loss=0.16, sim_loss=0.136, recon_loss=0.0173, kl_los\n",
      "Epoch 5: 100%|█| 30/30 [00:12<00:00,  2.44batch/s, loss=0.145, sim_loss=0.123, recon_loss=0.0152, kl_lo\n",
      "Epoch 6: 100%|█| 30/30 [00:12<00:00,  2.40batch/s, loss=0.125, sim_loss=0.104, recon_loss=0.0138, kl_lo\n",
      "Epoch 7: 100%|█| 30/30 [00:12<00:00,  2.33batch/s, loss=0.121, sim_loss=0.101, recon_loss=0.0127, kl_lo\n",
      "Epoch 8: 100%|█| 30/30 [00:12<00:00,  2.34batch/s, loss=0.108, sim_loss=0.0889, recon_loss=0.0116, kl_l\n",
      "Epoch 9: 100%|█| 30/30 [00:12<00:00,  2.36batch/s, loss=0.0986, sim_loss=0.0806, recon_loss=0.0109, kl_\n",
      "Epoch 10: 100%|█| 30/30 [00:11<00:00,  2.57batch/s, loss=0.0922, sim_loss=0.0746, recon_loss=0.0104, kl\n",
      "Epoch 11:  93%|▉| 28/30 [00:11<00:00,  2.37batch/s, loss=0.0863, sim_loss=0.0688, recon_loss=0.0103, kl"
     ]
    }
   ],
   "source": [
    "train=True\n",
    "if train:\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=INIT_LR)\n",
    "    #optimizer=torch.optim.SGD(model.parameters(),lr=INIT_LR,momentum=0.9)\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=20,gamma=0.2)\n",
    "\n",
    "    loss_history,recon_loss_history,kl_loss_history,sim_loss_history=model.fit(train_dataloader=train_dataloader,val_dataloader=test_dataloader,epochs=EPOCHS,optimizer=optimizer,device=device,wandb_log=False,save_model=base_path,early_stop=10,scheduler=scheduler)\n",
    "\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"models/vaesim_mnist/model.pt\"))\n",
    "    model.basis=torch.load(\"models/vaesim_mnist/vaesim_basis.pt\")\n",
    "    model=model.to(device)\n",
    "    model.device=device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04324ac8-1d11-4309-88e7-2a604cea3431",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c2c38-b82a-4575-bb0b-58825def899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_train=[]\n",
    "y_train=[]\n",
    "z_train=[]\n",
    "\n",
    "\n",
    "cl_test=[]\n",
    "y_test=[]\n",
    "z_test=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x,y in tqdm.tqdm(train_dataloader):\n",
    "        x=x.to(device)\n",
    "        z_mu,z_sigma=model.encoder(x)\n",
    "        dist=D.normal.Normal(z_mu, torch.exp(0.5 * z_sigma))\n",
    "        z=dist.sample()\n",
    "        sim=model.compute_similarity(z)\n",
    "        soft_sim=softmax(sim,dim=1)\n",
    "        cl_train.append(soft_sim.argmax(dim=1).cpu().numpy())\n",
    "        z_train.append(z.cpu().numpy())\n",
    "        y_train.append(y.cpu().numpy())\n",
    "        \n",
    "    for x,y in tqdm.tqdm(test_dataloader):\n",
    "        x=x.to(device)\n",
    "        z_mu,z_sigma=model.encoder(x)\n",
    "        dist=D.normal.Normal(z_mu, torch.exp(0.5 * z_sigma))\n",
    "        z=dist.sample().cpu()\n",
    "        sim=model.compute_similarity(z)\n",
    "        soft_sim=softmax(sim,dim=1)\n",
    "        cl_test.append(soft_sim.argmax(dim=1).cpu().numpy())\n",
    "        \n",
    "        z_test.append(z.cpu().numpy())\n",
    "        y_test.append(y.cpu().numpy())\n",
    "        \n",
    "    cl_train=np.concatenate(cl_train,0)\n",
    "    z_train=np.concatenate(z_train,0)\n",
    "    y_train=np.concatenate(y_train,0)\n",
    "\n",
    "    cl_test=np.concatenate(cl_test,0)\n",
    "    z_test=np.concatenate(z_test,0)\n",
    "    y_test=np.concatenate(y_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b471093-8a01-4884-b64b-641d29779c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"name\":\"VAESIM\",\"dataset\":\"MNIST\",\"n_cluster\":40}\n",
    "wandb.init(project=\"VAESIM_CHARACT\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a051980-6724-4e33-8a35-58cbd8ed2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=model_evaluation(z_train,y_train,z_test,y_test,cl_train,cl_test,device=\"cuda\",n_cluster=40)\n",
    "wandb.log(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0d042-6930-4b79-9833-ef05a82754b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch=next(iter(test_dataloader))\n",
    "\n",
    "# model.evaluate(batch,train_dataloader,test_dataloader,wandb_log=True,n_semi=6,lin_epochs=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606a909-f896-4e45-b667-a78ffde4e25b",
   "metadata": {},
   "source": [
    "## FINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a79b5-0836-40a4-8dca-a8296782cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_history,label=\"loss\")\n",
    "# plt.plot(recon_loss_history,label=\"recon_loss\")\n",
    "# plt.plot(kl_loss_history,label=\"kl_loss\")\n",
    "# plt.plot(sim_loss_history,label=\"sim_loss\")\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4522f3e-d718-475b-a5d0-34362621d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for x,y in test_dataloader:\n",
    "#     x2,z,s=model(x.to(device))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c9c6a-913f-4a9f-9383-52d82efb13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.basis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10fea1-df53-4187-b51b-c5958cf03138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,axs=plt.subplots(2,20,figsize=(15,2))\n",
    "\n",
    "# for i in range(20):\n",
    "#     axs[0,i].imshow(x[i].permute(1,2,0))\n",
    "#     axs[1,i].imshow(x2[i].detach().permute(1,2,0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96549a8-b283-49aa-b558-1b01e25ba2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
