{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca0e9c5-aa6e-4800-8490-43d0d4d3adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteoferrante\u001b[0m (\u001b[33mtorvergatafmri\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../NeuroGEN_Pytorch/\")\n",
    "from classes.VAE import VAE\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose,ToTensor,Resize,PILToTensor\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch import distributions as D\n",
    "import wandb\n",
    "\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "wandb.login()\n",
    "\n",
    "import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from evaluations import *\n",
    "\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8d76f3-d96e-4966-a90f-174670a629d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=50,target_shape=(1,28,28) , n_conv=2, n_init_filters=64, condition_dim=10):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.condition_dim=condition_dim\n",
    "\n",
    "\n",
    "\n",
    "        # infer the starting dimension.\n",
    "        target_shape_side = target_shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "        self.startDim = target_shape_side // (2 ** n_conv)\n",
    "\n",
    "        self.n_init_filters=n_init_filters\n",
    "        \n",
    "        #self.predecoder=nn.Unflatten(self.first_channels*self.startDim*self.startDim)\n",
    "        self.predecoder=nn.Linear(latent_dim,self.n_init_filters*self.startDim*self.startDim)\n",
    "        self.unflatten=nn.Unflatten(1,(self.n_init_filters,self.startDim,self.startDim))\n",
    "\n",
    "        #self.condition =  nn.Linear(self.condition_dim,self.startDim*self.startDim)\n",
    "        #self.condition2shape = nn.Unflatten(1, (1,self.startDim , self.startDim))\n",
    "        feature_layers = []\n",
    "        for i in range(n_conv):\n",
    "            if i==0:\n",
    "                feature_layers.append(nn.LazyConvTranspose2d(n_init_filters,kernel_size=4,stride=2,padding=1))\n",
    "            else:\n",
    "                feature_layers.append(nn.ConvTranspose2d(n_init_filters*(2**(i-1)),n_init_filters*2**i,kernel_size=4,stride=2,padding=1))\n",
    "\n",
    "        self.features = nn.Sequential(*feature_layers)\n",
    "\n",
    "        self.decoder_output=nn.LazyConvTranspose2d(target_shape[0],3,padding=1)\n",
    "        self.activation=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.predecoder(x)\n",
    "        x= self.unflatten(x)\n",
    "\n",
    "#         c= self.condition(c)\n",
    "#         c= self.condition2shape(c)\n",
    "        \n",
    "#         x= torch.concat((x,c),axis=1)\n",
    "        x = x.view(x.shape[0], -1, self.startDim, self.startDim)\n",
    "        x = self.features(x)\n",
    "        x = self.decoder_output(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86422a5-7125-4511-aaa7-38e9b0ee4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "encoder_architecture=[[0,64],[0,128],[0,256]]\n",
    "decoder_architecture=[[0,64],[0,128]]\n",
    "\n",
    "latent_dim=32\n",
    "input_dim=(1,32,32)\n",
    "\n",
    "\n",
    "config={\"dataset\":\"MNIST\", \"type\":\"VAE\",\"encoder_architecture\":encoder_architecture,\"decoder_architecture\":decoder_architecture}\n",
    "config[\"latent_dim\"]=latent_dim\n",
    "config[\"input_dim\"]=input_dim\n",
    "\n",
    "d=decoder(latent_dim=latent_dim,n_init_filters=128,target_shape=input_dim)\n",
    "\n",
    "\n",
    "model=VAE(input_dim=input_dim,latent_dim=latent_dim,encoder_architecture=encoder_architecture,decoder_architecture=decoder_architecture)\n",
    "model.decoder=d\n",
    "#model=VAE(input_dim,latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff91c2a-ff13-4986-94ec-1e97d5893a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_dir=r\"C:\\Users\\matte\\Dataset\\tor_vergata\\Dataset\\Img\\img_align_celeba\"\n",
    "#images_dir=r\"/home/matteo/NeuroGEN/Dataset/Img/img_align_celeba\"\n",
    "\n",
    "#other important definitions\n",
    "\n",
    "EPOCHS=20\n",
    "BS=2000\n",
    "INIT_LR=1e-4\n",
    "\n",
    "config[\"epochs\"]=EPOCHS\n",
    "config[\"BS\"]=BS\n",
    "config[\"init_lr\"]=INIT_LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e2061a-d0fe-4179-91d2-b967f2ea3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           1,088\n",
      "      ConvResBlock-2           [-1, 64, 16, 16]               0\n",
      "            Conv2d-3            [-1, 128, 8, 8]         131,200\n",
      "      ConvResBlock-4            [-1, 128, 8, 8]               0\n",
      "            Conv2d-5            [-1, 256, 4, 4]         524,544\n",
      "      ConvResBlock-6            [-1, 256, 4, 4]               0\n",
      "           Flatten-7                 [-1, 4096]               0\n",
      "            Linear-8                   [-1, 32]         131,104\n",
      "            Linear-9                   [-1, 32]         131,104\n",
      "================================================================\n",
      "Total params: 919,040\n",
      "Trainable params: 919,040\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.47\n",
      "Params size (MB): 3.51\n",
      "Estimated Total Size (MB): 3.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.encoder,(1,32,32),device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8579253-5f6b-48cc-beeb-ce2eca6952cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 8192]         270,336\n",
      "         Unflatten-2            [-1, 128, 8, 8]               0\n",
      "   ConvTranspose2d-3          [-1, 128, 16, 16]         262,272\n",
      "   ConvTranspose2d-4          [-1, 256, 32, 32]         524,544\n",
      "   ConvTranspose2d-5            [-1, 1, 32, 32]           2,305\n",
      "           Sigmoid-6            [-1, 1, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 1,059,457\n",
      "Trainable params: 1,059,457\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.39\n",
      "Params size (MB): 4.04\n",
      "Estimated Total Size (MB): 6.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.decoder,(latent_dim,),device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4186c2-c31e-4fb7-8e63-b100c3045272",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232be7fb-284f-4afa-94b8-cc1ae520500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz\n",
      "Using downloaded and verified file: /home/matteo/.medmnist/pneumoniamnist.npz\n"
     ]
    }
   ],
   "source": [
    "transform=Compose([Resize((32,32)),ToTensor()])\n",
    "\n",
    "data_flag = 'pneumoniamnist'\n",
    "# data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "train_dataset = DataClass(split='train', transform=transform, download=download)\n",
    "val_dataset = DataClass(split='val', transform=transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=transform, download=download)\n",
    "\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=BS,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,batch_size=BS,shuffle=True)\n",
    "\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=BS,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2362759-4ac7-4edb-9f83-bc3ccd3e7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(train_dataset,batch_size=BS,shuffle=True)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=BS,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa1c27-55e2-4889-955b-7dfead5b49a8",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bc6ddf-c136-4339-863a-03416adef76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"models/vae_pneumonia_km\"\n",
    "os.makedirs(base_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b6ddd5-b311-4d6f-8986-4964f24aac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24970a18-2b2f-439e-b36b-676c34a19444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"NeuroGEN_Pytorch\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6785672e-53ce-4e82-9def-812b892250da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                | 0/3 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.95 GiB (GPU 0; 47.54 GiB total capacity; 549.32 MiB already allocated; 1.39 GiB free; 656.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mINIT_LR)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.ckp\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/Unsupervised/vaesim_baselines/VAE_KMeans/../../NeuroGEN_Pytorch/classes/VAE.py:303\u001b[0m, in \u001b[0;36mVAE.fit\u001b[0;34m(self, train_dataloader, val_dataloader, epochs, optimizer, device, wandb_log, save_model, early_stop)\u001b[0m\n\u001b[1;32m    300\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m x_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m loss, recon_loss, kl_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(x_pred, x)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# backpropagate\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Unsupervised/vaesim_baselines/VAE_KMeans/../../NeuroGEN_Pytorch/classes/VAE.py:217\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m ref_sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_z \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39mnormal\u001b[38;5;241m.\u001b[39mNormal(ref_mu\u001b[38;5;241m.\u001b[39mto(device), ref_sigma\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m--> 217\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mdecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         c= self.condition(c)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#         c= self.condition2shape(c)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         \n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#         x= torch.concat((x,c),axis=1)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstartDim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstartDim)\n\u001b[0;32m---> 47\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_output(x)\n\u001b[1;32m     49\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:925\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# One cannot replace List by Tuple or Sequence in \"_output_padding\" because\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.95 GiB (GPU 0; 47.54 GiB total capacity; 549.32 MiB already allocated; 1.39 GiB free; 656.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "#model.load_state_dict(torch.load(r\"models/vae/model.ckp\"))\n",
    "train=True\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=INIT_LR)\n",
    "\n",
    "if train:\n",
    "    model.fit(train_dataloader=train_dataloader,val_dataloader=test_dataloader,epochs=EPOCHS,optimizer=optimizer,device=device,wandb_log=False,save_model=base_path,early_stop=3)\n",
    "    \n",
    "else:\n",
    "    model.load_state_dict(torch.load(os.path.join(base_path,\"model.ckp\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e623c-0d63-4542-a7fc-8797ce93b985",
   "metadata": {},
   "source": [
    "# BASELINE EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ab124-65b8-4c1e-b1db-9721aed04333",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_train=[]\n",
    "y_train=[]\n",
    "z_train=[]\n",
    "\n",
    "\n",
    "cl_test=[]\n",
    "y_test=[]\n",
    "z_test=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x,y in tqdm.tqdm(train_dataloader):\n",
    "        x=x.to(device)\n",
    "        z_mu,z_sigma=model.encoder(x)\n",
    "        dist=D.normal.Normal(z_mu, torch.exp(0.5 * z_sigma))\n",
    "        z=dist.sample().cpu()\n",
    "        \n",
    "        z_train.append(z.numpy())\n",
    "        y_train.append(y.numpy())\n",
    "        \n",
    "    for x,y in tqdm.tqdm(test_dataloader):\n",
    "        x=x.to(device)\n",
    "        z_mu,z_sigma=model.encoder(x)\n",
    "        dist=D.normal.Normal(z_mu, torch.exp(0.5 * z_sigma))\n",
    "        z=dist.sample().cpu()\n",
    "        \n",
    "        z_test.append(z.numpy())\n",
    "        y_test.append(y.numpy())\n",
    "        \n",
    "    #cl_train=np.concatenate(cl_train,0)\n",
    "    z_train=np.concatenate(z_train,0)\n",
    "    y_train=np.concatenate(y_train,0)\n",
    "\n",
    "    #cl_test=np.concatenate(cl_test,0)\n",
    "    z_test=np.concatenate(z_test,0)\n",
    "    y_test=np.concatenate(y_test,0)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fa3d7-10f7-4659-b7c3-05d87db84a27",
   "metadata": {},
   "source": [
    "## Looking for best kmeans K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2de290-8409-4ac2-950f-884d2bac4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel = KMeans()\n",
    "visualizer = KElbowVisualizer(kmodel, k=[5,10,15,20,25,30,35,40,50])\n",
    "\n",
    "visualizer.fit(z_train)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332e1c6-c6d4-4a7a-8e07-2070bd81e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=visualizer.elbow_value_\n",
    "kmodel=KMeans(n_clusters=n_clusters)\n",
    "kmodel.fit(z_train)\n",
    "\n",
    "cl_train=kmodel.predict(z_train)\n",
    "cl_test=kmodel.predict(z_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3d582-313b-4697-9eb0-c6c7179371e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"name\":\"VAEKMEANS\",\"dataset\":\"pneumonia\",\"n_cluster\":n_clusters}\n",
    "wandb.init(project=\"VAESIM_CHARACT\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee34b63-b3d4-4e57-86a8-2ef48938f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=model_evaluation(z_train,y_train,z_test,y_test,cl_train,cl_test,device=\"cuda\",n_cluster=n_clusters)\n",
    "wandb.log(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3aa7a-e1d7-4376-9f53-41c35d4e26a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
